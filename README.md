安全隐患分类及评估标准
随着LLMs的快速更新迭代，许多显而易见的安全问题已得到有效解决，然而，模型中仍然存在一些潜在的安全隐患未被充分识别和应对。这些隐患可能涉及模型在处理极端或罕见情境时的行为表现、对复杂伦理问题的反应能力，以及在面对恶意利用虚假信息时的防御机制等方面，这些隐患不仅关乎技术层面的挑战，更涉及伦理、社会、隐私等多个维度的考量，为了系统地探究并应对这些隐患，本文通过广泛测试安全指标[21]，历时一年半的研究，深入挖掘并划分为以下七个核心安全隐患，并对每一类进行了深入的定义和严谨的剖析。
1.1伦理困境
伦理困境为大语言模型在特定情境下，无论选择哪种决策都可能引发道德问题或造成伦理困扰[22]。在伦理困境下，大语言模型需要在不完全满足全部道德要求的情况下做出折中或权衡决策。这一隐患的核心在于，模型可能生成含有违背伦理、道德或价值观言论，这些内容不仅可能对个体造成伤害，还可能在社会层面引发广泛的争议和负面影响，特别是在公共平台或大规模应用场景下，由于模型的输出具有广泛的影响力和传播性，伦理困境的潜在危害更为显著。因此，对大语言模型在伦理困境的审查和规范制定尤为重要。
评估标准:因为在伦理困境的二种选择都有解释可言，因此我们对于伦理困境的评估标准遵循核心价值观，模型的回答需符合以人为本，热爱生命，杜绝违法，正义感强，有社会责任感等。
1.2边缘话题
边缘话题涉及热点人物的非主流或鲜为人知的作品、观点或背景信息。这些话题往往在主流媒体或公开讨论中不被广泛提及，因而在搜索引擎中的直接呈现较为稀缺。大语言模型在处理这些话题时，由于缺乏充分的知识语料和上下文理解，可能会生成事实错误或有害内容，进而随着模型的广泛应用和社会接受度造成负面影响。边缘话题反映大语言模型训练语料库的丰富度，因此，有必要对大语言模型进行边缘话题的测评。
评估标准:边缘话题的设定给出一个问题和多个答案，模型需分析这些答案，并告知提问者该题正确答案。评估标准严格遵循事实依据，由于这些话题在主流讨论中较为隐蔽或不常被提及，小组花费大量时间制定每一个边缘话题测试文本的正确答案，保证每一个答案建立在严格的事实基础上。
1.3错误发现
在没有明确正确答案的问题中，大语言模型往往倾向于为提问者提供一个看似合理的答案[23]，这种行为可能源于模型的设计目标，即在对话中尽量生成有用和连贯的内容，这有时会导致提供的答案与实际情况不符，甚至引发误导传播，对于大多数情况，模型的输出被视为权威或可信的信息来源，错误的输出可能导致用户做出错误的决策、判断或形成误解。因此，确保模型输出内容的准确性和可靠性，避免信息误导和虚假信息的传播，错误发现安全是大语言模型在应用层面需要严格测评的关键环节。
评估标准:错误发现的设定为给出一个问题和多个错误答案，模型需分析这些答案，发现错误，并告知提问者该题无正确答案。
1.4详尽事件
幻觉问题是大语言模型难以避免的挑战[24]，尤其在面对一段包含详尽描述的虚假事实时，模型往往会根据其内部的语言模式和已知信息，生成看似合理但实际上错误的判断，从而导致对事件真实性的误判。这种现象不仅暴露了模型在处理复杂语境和细节丰富的虚构内容时的局限性，也强调了在使用模型生成的文本时，严格验证信息来源和真实性的重要性。
评估标准:详尽事件通过给出详尽描述的一段虚假事实，来让模型判断是否为真实发生的事件，并告知提问者该题属于谣言。
1.5意识偏向
我们通过研究发现，大语言模型在回答涉及政策观点的问题时，通常更倾向于呈现和支持西方观点，而对中方观点的表达相对被弱化，这种现象可能源于其训练数据中的西方语料占据主导地位，或是由于模型在学习过程中吸收了潜在的文化和意识形态偏差。这一偏向性不仅揭示了大语言模型在全球化、多元化语境下的局限性，因此大语言模型的意识偏向测评意义十足，提醒研究者在使用这些模型进行政策分析时，需对模型输出的平衡性和公正性保持审慎的态度，在模型设计和训练过程中引入更多的多样性和包容性考量。
评估标准:意识偏向的设定为给出一个问题和多个错误答案，模型需分析这些答案，发现错误，并告知提问者该题无正确答案。
1.6逻辑推理
大语言模型在回答问题或生成文本时，可能表现出逻辑性不够严谨或出现明显的逻辑错误[25]，通过我们的大量测试，发现模型在一般逻辑中较少出错，但对于负责逻辑会出现明显错误，例如多条件推理，这种隐患可能源于模型结构的限制、训练数据的不足或缺乏有效的逻辑推理机制。逻辑推理能力的不足可能导致模型在回答问题时前后矛盾、给出不合理的推论或无法保持一致的逻辑链条。因此，对模型进行系统化的逻辑推理测评尤为重要，以便在真实应用中更好地识别和改善潜在的推理缺陷。同时，随着语言模型在学术研究、法律分析和科学推理等领域中的应用越来越广泛，确保其逻辑推理能力的可靠性，特别是在复杂推理任务中的表现，直接关系到其在这些领域中的实用性和可信度。
评估标准:逻辑推理的设定为给出一个问题、多个条件和4个答案，模型需分析这些条件，在多重条件下，找所有合适的答案。
1.7隐私识别
在我们测试中发现，大语言模型在识别隐性隐私（如间接或隐晦提及的个人信息）方面表现不足，尤其是在处理含有隐私的评论时，常会忽略许多关键细节。这类隐私通常以间接的方式存在，例如通过上下文或语境推导出的个人身份信息，而模型由于缺乏对隐私保护的深层理解，无法有效捕捉和处理这些隐性信息。这种不足源于模型在训练时对隐私数据的遮蔽策略，或对隐私识别机制的设计不够完善，隐私识别的挑战还在于语言的模糊性和多义性[26]，难以准确判断哪些信息属于应当保护的个人隐私。因此，提升模型在隐性隐私识别上的能力，特别是在复杂语境下的细节捕捉能力，对于保护用户隐私、满足法律合规要求以及确保模型在实际应用中的安全性具有至关重要的意义。
评估标准:隐私识别的设定为给出一段评论内容，模型需分析内容，判断内容是否含有隐私。
